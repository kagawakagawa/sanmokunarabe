{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6s9TpgtupzXX"
   },
   "outputs": [],
   "source": [
    "from game import State\r\n",
    "from dual_network import DN_INPUT_SHAPE\r\n",
    "from dual_network import dual_network\r\n",
    "from math import sqrt\r\n",
    "from tensorflow.keras.models import load_model\r\n",
    "from pathlib import Path\r\n",
    "import numpy as np\r\n",
    "#import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z4YG01BcuOGT"
   },
   "outputs": [],
   "source": [
    "# パラメータの準備\r\n",
    "PV_EVALUATE_COUNT = 50 # 1推論当たりのシミュレーション回数（本家は1600）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oTE0dXDSuujb"
   },
   "outputs": [],
   "source": [
    "# 推論\r\n",
    "def predict(model,state):\r\n",
    "  # 推論のための入力データのシェイプの変換\r\n",
    "  a, b, c = DN_INPUT_SHAPE\r\n",
    "  x = np.array([state.pieces,state.enemy_pieces])\r\n",
    "  x = x.reshape(c,a,b).transpose(1,2,0).reshape(1,a,b,c)\r\n",
    "\r\n",
    "  # 推論\r\n",
    "  y = model.predict(x,batch_size=1)\r\n",
    "\r\n",
    "  # 方策の取得\r\n",
    "  policies = y[0][0][list(state.legal_actions())] # 合法手のみ\r\n",
    "  policies /= sum(policies) if sum(policies) else 1 # 合計1の確率分布に変換\r\n",
    "\r\n",
    "  # 価値の取得\r\n",
    "  value = y[1][0][0]\r\n",
    "  return policies,value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dLX0TSgOwnnm"
   },
   "outputs": [],
   "source": [
    "# ノードのリストを試行回数のリストに変換\r\n",
    "def nodes_to_scores(nodes):\r\n",
    "  scores = []\r\n",
    "  for c in nodes:\r\n",
    "    scores.append(c.n)\r\n",
    "  return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ts-wKtZsw7jT"
   },
   "outputs": [],
   "source": [
    "# モンテカルロ木探索のスコアの取得\r\n",
    "def pv_mcts_scores(model,state,temperature):\r\n",
    "  # モンテカルロ木探索のノードの定義\r\n",
    "  class node:\r\n",
    "    # ノードの初期化\r\n",
    "    def __init__(self,state,p):\r\n",
    "      self.state = state # 状態\r\n",
    "      self.p = p # 方策\r\n",
    "      self.w = 0 # 累計価値\r\n",
    "      self.n = 0 # 試行回数\r\n",
    "      self.child_nodes = None # 子ノード群\r\n",
    "    \r\n",
    "    # 局面の価値の計算\r\n",
    "    def evaluate(self):\r\n",
    "      # ゲーム終了時\r\n",
    "      if self.state.is_done():\r\n",
    "        # 勝敗結果で価値を取得\r\n",
    "        value = -1 if self.state.is_lose() else 0\r\n",
    "\r\n",
    "        # 累計価値と試行回数の更新\r\n",
    "        self.w += value\r\n",
    "        self.n += 1\r\n",
    "        return value\r\n",
    "\r\n",
    "      # 子ノードが存在しない時\r\n",
    "      if not self.child_nodes:\r\n",
    "        # ニューラルネットワークの推論で豊作と価値を取得\r\n",
    "        policies,value = predict(model,self.state)\r\n",
    "        \r\n",
    "        # 累計価値と試行回数の更新\r\n",
    "        self.w += value\r\n",
    "        self.n += 1\r\n",
    "\r\n",
    "        # 子ノードの展開\r\n",
    "        self.child_nodes = []\r\n",
    "        for action, policy in zip(self.state.legal_actions(),policies):\r\n",
    "          self.child_nodes.append(node(self.state.next(action),policy))\r\n",
    "        return value\r\n",
    "\r\n",
    "      # 子ノードが存在するとき\r\n",
    "      else:\r\n",
    "        # アーク評価値が最大の子ノードの評価で価値を取得\r\n",
    "        value = -self.next_child_node().evaluate()\r\n",
    "\r\n",
    "        # 累計価値と試行回数の更新\r\n",
    "        self.w += value\r\n",
    "        self.n += 1\r\n",
    "        return value\r\n",
    "      \r\n",
    "    # アーク評価値が最大の子ノードを取得\r\n",
    "    def next_child_node(self):\r\n",
    "      # アーク評価値\r\n",
    "      C_PUCT = 1.0\r\n",
    "      t = sum(nodes_to_scores(self.child_nodes))\r\n",
    "      pucb_values = []\r\n",
    "      for child_node in self.child_nodes:\r\n",
    "        pucb_values.append((-child_node.w / child_node.n if child_node.n else 0.0) + C_PUCT * child_node.p * sqrt(t) / (1 + child_node.n))\r\n",
    "\r\n",
    "      # アーク評価値が最大の子ノードを返す\r\n",
    "      return self.child_nodes[np.argmax(pucb_values)]\r\n",
    "      \r\n",
    "\r\n",
    "  # 現在の局面のノードの作成\r\n",
    "  root_node = node(state,0)\r\n",
    "\r\n",
    "  # 複数回の評価を実行\r\n",
    "  for _ in range(PV_EVALUATE_COUNT):\r\n",
    "    root_node.evaluate()\r\n",
    "\r\n",
    "  # 合法手の確率分布\r\n",
    "  scores = nodes_to_scores(root_node.child_nodes)\r\n",
    "  if temperature == 0: # 最大値のみ1\r\n",
    "    action = np.argmax(scores)\r\n",
    "    scores = np.zeros(len(scores))\r\n",
    "    scores[action] = 1\r\n",
    "  else: # ボルツマン分布でバラつき付加\r\n",
    "    scores = boltzman(scores,temperature)\r\n",
    "  return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K4Dkq8I0zKEP"
   },
   "outputs": [],
   "source": [
    "# モンテカルロ木探索で行動選択\r\n",
    "def pv_mcts_action(model,temperature=0):\r\n",
    "  def pv_mcts_action(state):\r\n",
    "    scores = pv_mcts_scores(model,state,temperature)\r\n",
    "    return np.random.choice(state.legal_actions(),p=scores)\r\n",
    "  return pv_mcts_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6c08_xV_GtJt"
   },
   "outputs": [],
   "source": [
    "# ボルツマン分布\r\n",
    "def boltzman(xs,temperature):\r\n",
    "  xs = [x**(1/temperature) for x in xs]\r\n",
    "  return [x / sum(xs) for x in xs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10143,
     "status": "ok",
     "timestamp": 1615307738749,
     "user": {
      "displayName": "天パ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgcJ5QhEN1QkiiHNRKp9yZUn-Oekyjo7_nsFxvaeg=s64",
      "userId": "16910291732707859644"
     },
     "user_tz": -540
    },
    "id": "FnuSP14UHDBT",
    "outputId": "70c81778-2d47-444a-c2ce-36c54cfec16e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "---\n",
      "-o-\n",
      "---\n",
      "\n",
      "---\n",
      "-o-\n",
      "-x-\n",
      "\n",
      "-o-\n",
      "-o-\n",
      "-x-\n",
      "\n",
      "-ox\n",
      "-o-\n",
      "-x-\n",
      "\n",
      "oox\n",
      "-o-\n",
      "-x-\n",
      "\n",
      "oox\n",
      "-o-\n",
      "-xx\n",
      "\n",
      "oox\n",
      "-oo\n",
      "-xx\n",
      "\n",
      "oox\n",
      "xoo\n",
      "-xx\n",
      "\n",
      "oox\n",
      "xoo\n",
      "oxx\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 動作確認\r\n",
    "if __name__ == '__main__':\r\n",
    "  # モデルの読み込み\r\n",
    "  path = sorted(Path('./model').glob('*.h5'))[-1]\r\n",
    "  model = load_model(str(path))\r\n",
    "\r\n",
    "  # 状態の生成\r\n",
    "  state = State()\r\n",
    "\r\n",
    "  # モンテカルロ木探索で行動取得を行う関数の生成\r\n",
    "  next_action = pv_mcts_action(model,1.0)\r\n",
    "\r\n",
    "  # ゲーム終了までループ\r\n",
    "  while True:\r\n",
    "    # ゲーム終了時\r\n",
    "    if state.is_done():\r\n",
    "      break\r\n",
    "    # 行動の取得\r\n",
    "    action = next_action(state)\r\n",
    "    # 次の状態の取得\r\n",
    "    state = state.next(action)\r\n",
    "    # 文字列表示\r\n",
    "    print(state)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM6EL8F7LohXaoIpp/ji7VV",
   "collapsed_sections": [],
   "name": "pv_mcts.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
